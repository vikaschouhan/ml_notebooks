{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from   modules.finance import *\n",
    "from   modules.utils import *\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_dir = 'out/returns_prediction_from_visual_patterns_notebook1__data'\n",
    "mkdir(work_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get historical data\n",
    "hist_data = load_pickle('data/data/nse_historical_25082020.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get keys for train & val\n",
    "keys = list(hist_data.keys())\n",
    "random.shuffle(keys)\n",
    "train_keys = keys[:int(0.8*len(keys))]\n",
    "val_keys   = keys[int(0.8*len(keys)):]\n",
    "\n",
    "# Define train and val dir\n",
    "train_dir  = os.path.join(work_dir, 'train')\n",
    "val_dir    = os.path.join(work_dir, 'val')\n",
    "train_annot_file = os.path.join(work_dir, 'train.csv')\n",
    "val_annot_file   = os.path.join(work_dir, 'val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Wrapper over generate_training_data_from_ticker\n",
    "def gen_train_data(key, out_dir, num_samples=20,\n",
    "        forward_period=4, backward_period=20, tick_name=None,\n",
    "        timeout=20):\n",
    "    # randomize number of samples\n",
    "    forward_period  = random.randint(num_samples, num_samples+200)\n",
    "    # randomize number of previous candles\n",
    "    backward_period = random.randint(backward_period, backward_period+40)\n",
    "    return generate_training_data_from_ticker(hist_data[key], out_dir=out_dir,\n",
    "                                             num_samples=num_samples, forward_period=forward_period,\n",
    "                                             backward_period=backward_period, tick_name=key,\n",
    "                                             timeout=timeout)\n",
    "# enddef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [02:31<00:00,  1.51s/it]\n",
      "100%|██████████| 100/100 [01:53<00:00,  1.13s/it]\n"
     ]
    }
   ],
   "source": [
    "# Generate multiple copies of train_keys selected at random\n",
    "train_keys2 = []\n",
    "for key_t in train_keys:\n",
    "    train_keys2 += random.randint(1, 15) * [key_t]\n",
    "# endfor\n",
    "random.shuffle(train_keys2)\n",
    "\n",
    "val_keys2 = []\n",
    "for key_t in val_keys:\n",
    "    val_keys2 += random.randint(1, 5) * [key_t]\n",
    "# endfor\n",
    "random.shuffle(val_keys2)\n",
    "\n",
    "# Generate data with train_keys2\n",
    "shutil.rmtree(train_dir, ignore_errors=True)\n",
    "train_annots = spawn(gen_train_data, train_keys2[:100], out_dir=train_dir)\n",
    "\n",
    "shutil.rmtree(val_dir, ignore_errors=True)\n",
    "val_annots = spawn(gen_train_data, val_keys2[:100], out_dir=val_dir)\n",
    "    \n",
    "def generate_annot_file(annot, out_file):\n",
    "    annot = {x:y for z in annot for x,y in z.items()}\n",
    "    annot = pd.DataFrame(annot.items())\n",
    "    annot.columns = ['image', 'return']\n",
    "    annot = annot.set_index('image')\n",
    "    annot.to_csv(out_file)\n",
    "# enddef\n",
    "\n",
    "# Generate annot files for train and val\n",
    "generate_annot_file(train_annots, train_annot_file) \n",
    "generate_annot_file(val_annots, val_annot_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from   tensorflow.keras.layers import Dense, ReLU\n",
    "from   tensorflow.keras import Model\n",
    "import glob\n",
    "\n",
    "####################################################\n",
    "# Create fn for tf.data ingestor pipeline\n",
    "####################################################\n",
    "# Parse one file record\n",
    "# record is a dictionary\n",
    "def parse_one_record(record):\n",
    "    # convert the compressed string to a 3D uint8 tensor\n",
    "    record['image'] = tf.image.decode_image(tf.io.read_file(record['image']), channels=3, expand_animations=False)\n",
    "    return record\n",
    "# enddef\n",
    "\n",
    "# Create dataset\n",
    "# image data is dictionary\n",
    "def _create_dataset(image_data):\n",
    "    dh_dataset = tf.data.Dataset.from_tensor_slices(image_data)\n",
    "    dh_dataset = dh_dataset.map(parse_one_record)\n",
    "    return dh_dataset\n",
    "# enddef\n",
    "\n",
    "\n",
    "# Create data ingestor for tensorflow-2.x\n",
    "def create_dataset(image_dir, annot_file):\n",
    "    # Get list of image paths and labels\n",
    "    metadata = pd.read_csv(annot_file, index_col=0)\n",
    "    # Convert image_data to format suitable for dataset ingestion\n",
    "    image_data         = {\n",
    "                             'image' : [os.path.join(image_dir, x) for x in metadata.index.to_list()],\n",
    "                             'label' : metadata[metadata.columns[0]].to_list(),\n",
    "                         }\n",
    "    # Add few more info\n",
    "    image_list         = image_data['image']\n",
    "    image_id_list      = [os.path.basename(x) for x in image_list] # Get ids\n",
    "    image_format_list  = [os.path.splitext(x)[1][1:] for x in image_id_list]\n",
    "    num_samples        = len(image_list)\n",
    "\n",
    "    image_data['imageformat']  = image_format_list\n",
    "    image_data['imageid']      = image_id_list\n",
    "    \n",
    "    # Populate dataset and few extra info\n",
    "    ret_value  = {}\n",
    "    # Dataset\n",
    "    ret_value['dataset']           = _create_dataset(image_data)\n",
    "    ret_value['num_samples']       = num_samples\n",
    "    return ret_value\n",
    "# enddef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "# Create fn for preprocessing pipeline.\n",
    "#####################################################\n",
    "def apply_image_normalization(image, normalize_type=0) :\n",
    "    if normalize_type == 0: \n",
    "        image = tf.subtract(image, 0.5)\n",
    "        image = tf.multiply(image, 2.0) # All pixels now between -1.0 and 1.0\n",
    "        return image\n",
    "    elif normalize_type == 1:\n",
    "        image = tf.multiply(image, 2.0) # All pixels now between 0.0 and 2.0\n",
    "        image = image - tf.reduce_mean(image, axis=[0, 1]) \n",
    "        # Most pixels should be between -1.0 and 1.0\n",
    "        return image\n",
    "    elif normalize_type == 2:\n",
    "        image = tf.image.per_image_standardization(image)\n",
    "        image = tf.multiply(image, 0.4) # This makes 98.8% of pixels between -1.0 and 1.0\n",
    "        return image\n",
    "    else :\n",
    "        raise ValueError('invalid value for normalize_type: {}'.format(normalize_type))\n",
    "    # endif\n",
    "# enddef\n",
    "\n",
    "def preprocess_image(image, size, normalize_type=2):\n",
    "    # Convert float32\n",
    "    if image.dtype != tf.float32:\n",
    "        image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    # endif\n",
    "    \n",
    "    # Resize to target size\n",
    "    image     = tf.image.resize(image, [size, size], method='bilinear', antialias=False)\n",
    "    # Apply normalization\n",
    "    image     = apply_image_normalization(image, normalize_type)\n",
    "    return image\n",
    "# enddef\n",
    "\n",
    "def create_preprocessing_function(image_size, normalize_type=2):\n",
    "    def __wrap(record):\n",
    "        record['image'] = preprocess_image(record['image'], image_size, normalize_type=normalize_type)\n",
    "        return record\n",
    "    # enddef\n",
    "\n",
    "    return __wrap\n",
    "# enddef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "# Create final data pipeline\n",
    "####################################\n",
    "tgt_image_size   = 224\n",
    "train_batch_size = 32\n",
    "\n",
    "# Generate pipeline\n",
    "train_dataset = create_dataset(train_dir, train_annot_file)['dataset']\n",
    "val_dataset   = create_dataset(val_dir, val_annot_file)['dataset']\n",
    "\n",
    "# Apply preprocessing\n",
    "prep_fn       = create_preprocessing_function(tgt_image_size)\n",
    "train_dataset = train_dataset.map(prep_fn)\n",
    "val_dataset   = val_dataset.map(prep_fn)\n",
    "\n",
    "# Map from dict to tuples\n",
    "train_dataset = train_dataset.map(lambda record: (record['image'], record['label']))\n",
    "val_dataset   = val_dataset.map(lambda record: (record['image'], record['label']))\n",
    "\n",
    "# Apply batch\n",
    "train_dataset = train_dataset.batch(train_batch_size)\n",
    "val_dataset   = val_dataset.batch(train_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "# Create a tensorflow model. Initialize it and train it\n",
    "##########################################\n",
    "model_inp_shape = (tgt_image_size, tgt_image_size, 3)\n",
    "model_t = tf.keras.applications.MobileNetV2(input_shape=model_inp_shape,\n",
    "                                            classes=1,\n",
    "                                            classifier_activation=None,\n",
    "                                            include_top=False,\n",
    "                                            pooling='max')\n",
    "# Change activation to linear instead of 'softmax', since it's a regression problem\n",
    "output = Dense(1, activation='linear')(model_t.output)\n",
    "\n",
    "# Create new model\n",
    "model_t = Model(inputs=model_t.input, outputs=output)\n",
    "# Dump summary\n",
    "#model_t.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model_t.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "61/61 [==============================] - 482s 8s/step - loss: 0.8710 - mae: 0.7362 - val_loss: 2.5819 - val_mae: 1.2215\n",
      "Epoch 2/2\n",
      "61/61 [==============================] - 417s 7s/step - loss: 0.8944 - mae: 0.7396 - val_loss: 2.3564 - val_mae: 1.2574\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd0c502b6d0>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run model\n",
    "model_t.fit(train_dataset, validation_data=val_dataset, epochs=2)"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
